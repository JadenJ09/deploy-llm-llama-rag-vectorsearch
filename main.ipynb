{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0/ Configuration for Data Preparation Using GoogleAPI and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (4.12.2)\n",
      "Requirement already satisfied: google-api-python-client in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (2.110.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from BeautifulSoup4) (2.5)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-api-python-client) (2.25.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-api-python-client) (0.1.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-api-python-client) (2.15.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.62.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.1)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install BeautifulSoup4 google-api-python-client python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class DocumentationBuilder:\n",
    "    def __init__(self, query, num_results):\n",
    "        self.google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        self.search_engine_id = os.getenv('SEARCH_ENGINE_ID')\n",
    "        self.query = query\n",
    "        self.num_results = num_results\n",
    "\n",
    "    def google_search(self):\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=self.google_api_key)\n",
    "        res = service.cse().list(q=self.query, cx=self.search_engine_id, num=self.num_results).execute()\n",
    "        return [item['link'] for item in res['items']]\n",
    "\n",
    "    def scrape_html(self, url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        return str(soup)\n",
    "\n",
    "    def build_documentation(self):\n",
    "        urls = self.google_search()\n",
    "        raw_documentation = []\n",
    "        for url in urls:\n",
    "            html_content = self.scrape_html(url)\n",
    "            raw_documentation.append({'url': url, 'text': html_content})\n",
    "        return raw_documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your query here\n",
    "query = 'Metaverse 2023'\n",
    "num_results = 3\n",
    "\n",
    "doc_builder = DocumentationBuilder(query, num_results)\n",
    "raw_documentation = doc_builder.build_documentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame and save, or use directly\n",
    "df = pd.DataFrame(raw_documentation)\n",
    "df.to_csv('raw_documentation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hbr.org/2023/05/yes-the-metaverse-is-s...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;!--[if IE 8]&gt;\\n&lt;html class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.forbes.com/sites/bernardmarr/2022/...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.pwc.com/us/en/tech-effect/innovati...</td>\n",
       "      <td>\\n&lt;!DOCTYPE HTML&gt;\\n\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://hbr.org/2023/05/yes-the-metaverse-is-s...   \n",
       "1  https://www.forbes.com/sites/bernardmarr/2022/...   \n",
       "2  https://www.pwc.com/us/en/tech-effect/innovati...   \n",
       "\n",
       "                                                text  \n",
       "0  <!DOCTYPE html>\\n\\n<!--[if IE 8]>\\n<html class...  \n",
       "1  <!DOCTYPE html>\\n<html lang=\"en\"><head><link a...  \n",
       "2  \\n<!DOCTYPE HTML>\\n\\n<html lang=\"en\">\\n<head>\\...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1/ Data preparation for LLM Chatbot RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/12/13 10:18:01 WARN Utils: Your hostname, JadenRazer resolves to a loopback address: 127.0.1.1; using 172.19.39.166 instead (on interface eth0)\n",
      "23/12/13 10:18:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/13 10:18:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SearchAI\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ ------------\n",
      "aiohttp                  3.9.1\n",
      "aiosignal                1.3.1\n",
      "annotated-types          0.6.0\n",
      "anyio                    4.1.0\n",
      "asttokens                2.4.1\n",
      "attrs                    23.1.0\n",
      "beautifulsoup4           4.12.2\n",
      "Bottleneck               1.3.5\n",
      "cachetools               5.3.2\n",
      "certifi                  2023.11.17\n",
      "charset-normalizer       3.3.2\n",
      "comm                     0.1.4\n",
      "dataclasses-json         0.6.3\n",
      "dbdemos                  0.3.43\n",
      "dbsqlclone               0.1.24\n",
      "debugpy                  1.6.7\n",
      "decorator                5.1.1\n",
      "exceptiongroup           1.2.0\n",
      "executing                2.0.1\n",
      "filelock                 3.13.1\n",
      "frozenlist               1.4.0\n",
      "fsspec                   2023.12.2\n",
      "google-api-core          2.15.0\n",
      "google-api-python-client 2.110.0\n",
      "google-auth              2.25.2\n",
      "google-auth-httplib2     0.1.1\n",
      "googleapis-common-protos 1.62.0\n",
      "greenlet                 3.0.2\n",
      "httplib2                 0.22.0\n",
      "huggingface-hub          0.19.4\n",
      "idna                     3.6\n",
      "importlib-metadata       7.0.0\n",
      "ipykernel                6.26.0\n",
      "ipython                  8.18.1\n",
      "jedi                     0.19.1\n",
      "jsonpatch                1.33\n",
      "jsonpointer              2.4\n",
      "jupyter_client           8.6.0\n",
      "jupyter_core             5.5.0\n",
      "langchain                0.0.349\n",
      "langchain-community      0.0.1\n",
      "langchain-core           0.0.13\n",
      "langsmith                0.0.69\n",
      "lxml                     4.9.3\n",
      "marshmallow              3.20.1\n",
      "matplotlib-inline        0.1.6\n",
      "mkl-fft                  1.3.8\n",
      "mkl-random               1.2.4\n",
      "mkl-service              2.4.0\n",
      "multidict                6.0.4\n",
      "mypy-extensions          1.0.0\n",
      "nest-asyncio             1.5.8\n",
      "numexpr                  2.8.7\n",
      "numpy                    1.26.2\n",
      "packaging                23.2\n",
      "pandas                   2.1.4\n",
      "parso                    0.8.3\n",
      "pexpect                  4.8.0\n",
      "pickleshare              0.7.5\n",
      "pip                      23.3.1\n",
      "platformdirs             4.1.0\n",
      "prompt-toolkit           3.0.41\n",
      "protobuf                 4.25.1\n",
      "psutil                   5.9.0\n",
      "ptyprocess               0.7.0\n",
      "pure-eval                0.2.2\n",
      "py4j                     0.10.9.7\n",
      "pyarrow                  11.0.0\n",
      "pyasn1                   0.5.1\n",
      "pyasn1-modules           0.3.0\n",
      "pydantic                 2.5.2\n",
      "pydantic_core            2.14.5\n",
      "Pygments                 2.17.2\n",
      "pyparsing                3.1.1\n",
      "pyspark                  3.5.0\n",
      "python-dateutil          2.8.2\n",
      "python-dotenv            1.0.0\n",
      "pytz                     2023.3.post1\n",
      "PyYAML                   6.0.1\n",
      "pyzmq                    25.1.0\n",
      "regex                    2023.10.3\n",
      "requests                 2.31.0\n",
      "rsa                      4.9\n",
      "safetensors              0.4.1\n",
      "setuptools               68.0.0\n",
      "six                      1.16.0\n",
      "sniffio                  1.3.0\n",
      "soupsieve                2.5\n",
      "SQLAlchemy               2.0.23\n",
      "stack-data               0.6.2\n",
      "tenacity                 8.2.3\n",
      "tokenizers               0.15.0\n",
      "tornado                  6.3.3\n",
      "tqdm                     4.66.1\n",
      "traitlets                5.14.0\n",
      "transformers             4.36.0\n",
      "typing_extensions        4.9.0\n",
      "typing-inspect           0.9.0\n",
      "tzdata                   2023.3\n",
      "uritemplate              4.1.1\n",
      "urllib3                  2.1.0\n",
      "wcwidth                  0.2.12\n",
      "wheel                    0.41.2\n",
      "yarl                     1.9.4\n",
      "zipp                     3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting HTML pages in smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Assuming df is your pandas DataFrame from the previous scraping step\n",
    "\n",
    "max_chunk_size = 500\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=[(\"h2\", \"header2\")])\n",
    "\n",
    "def split_html_on_h2(html, min_chunk_size = 20, max_chunk_size=500):\n",
    "  h2_chunks = html_splitter.split_text(html)\n",
    "  chunks = []\n",
    "  previous_chunk = \"\"\n",
    "  # Merge chunks together to add text before h2 and avoid too small docs.\n",
    "  for c in h2_chunks:\n",
    "    # Concat the h2 (note: we could remove the previous chunk to avoid duplicate h2)\n",
    "    content = c.metadata.get('header2', \"\") + \"\\n\" + c.page_content\n",
    "    if len(tokenizer.encode(previous_chunk + content)) <= max_chunk_size/2:\n",
    "        previous_chunk += content + \"\\n\"\n",
    "    else:\n",
    "        chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n",
    "        previous_chunk = content + \"\\n\"\n",
    "  if previous_chunk:\n",
    "      chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n",
    "  # Discard too small chunks\n",
    "  return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]\n",
    "\n",
    "# Register the UDF in Spark\n",
    "@pandas_udf(\"array<string>\")\n",
    "def parse_and_split(docs: pd.Series) -> pd.Series:\n",
    "    return docs.apply(split_html_on_h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame to a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Apply the UDF to the 'text' column\n",
    "chunked_documents = spark_df.withColumn(\"chunked_text\", parse_and_split(spark_df[\"text\"]))\n",
    "\n",
    "# Now you can work with the chunked documents in your Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 url|                text|\n",
      "+--------------------+--------------------+\n",
      "|https://hbr.org/2...|<!DOCTYPE html>\\n...|\n",
      "|https://www.forbe...|<!DOCTYPE html>\\n...|\n",
      "|https://www.pwc.c...|\\n<!DOCTYPE HTML>...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[url: string, text: string, chunked_text: array<string>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2568 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2152 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                 url|                text|        chunked_text|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|https://hbr.org/2...|<!DOCTYPE html>\\n...|[Navigation Menu ...|\n",
      "|https://www.forbe...|<!DOCTYPE html>\\n...|[Subscribe to new...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "chunked_documents.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/miniconda3/envs/deploy-llm-llama-rag-vectorsearch/lib/python3.12/site-packages (4.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the first row's HTML content\n",
    "html_row = spark_df.select(\"text\").limit(1).collect()\n",
    "html_content = html_row[0]['text']\n",
    "\n",
    "# Apply the chunking function\n",
    "chunks = split_html_on_h2(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Navigation Menu  \\nSubscribe Sign In Search Menu  \\nAccount Menu  \\nAccount Menu  \\nHi,  \\n\\xa0Guest  \\nClose menu  \\nSearch CLEAR  \\nSUGGESTED TOPICS  \\nExplore HBR  \\nLatest The Magazine Ascend Podcasts Video Store Webinars Newsletters  \\nPopular Topics  \\nManaging Yourself Leadership Strategy Managing Teams Gender Innovation Work-life Balance All Topics  \\nFor Subscribers  \\nThe Big Idea Data & Visuals Reading Lists Case Selections HBR Learning Subscribe  \\nMy Account  \\nMy Library Topic Feeds Orders Account Settings Email Preferences Log Out Sign In  \\nSubscribe Latest Podcasts Video The Magazine Ascend Store Webinars Newsletters All Topics The Big Idea Data & Visuals Reading Lists Case Selections HBR Learning My Library Account Settings Log Out Sign In  \\nYour Cart  \\nVisit Our Store  \\nYour Shopping Cart is empty.  \\nGuest User  \\nSubscriber  \\nMy Library Topic Feeds Orders Account Settings Email Preferences Log Out  \\nReading List  \\nReading Lists  \\nYou have 1 free articles left this month.  \\nYou are reading your last free article for this month.  \\nSubscribe for unlimited access.  \\nCreate an account to read 2 more.  \\nWeb-based technologies  \\nYes, the Metaverse Is Still Happening  \\nDon’t get left behind while competitors continue to invest in virtual environments.  \\nby  \\nDeborah Perry Piscione and Josh Drean  \\nby  \\nDeborah Perry Piscione and Josh Drean',\n",
       " 'by  \\nDeborah Perry Piscione and Josh Drean  \\nby  \\nDeborah Perry Piscione and Josh Drean  \\nMay 12, 2023  \\nHBR Staff/Pexels  \\nTweet  \\nPost  \\nShare  \\nAnnotate  \\nSave  \\nGet PDF  \\nBuy Copies  \\nPrint  \\nSummary.\\xa0\\xa0\\xa0  \\nThere’s still much hype around the metaverse. And although no one knows how things will shape out, the metaverse is happening and companies need to develop a strategy. Currently, large enterprises such as NVIDIA and Unity are investing heavily to lay the foundational infrastructure, while Roblox, Decentraland, and Sandbox are jockeying to be the preferred portal, and Web3 studios such as Touchcast and TerraZero are working with leading brands to expand their market share. Now is the time to discover the metaverse and its power to drive deeper connections, more effective collaboration, and enhanced personal productivity and fulfillment.  \\n\\xa0  \\nTweet  \\nPost  \\nShare  \\nAnnotate  \\nSave  \\nGet PDF  \\nBuy Copies  \\nPrint  \\nWhile artificial intelligence continues to dominate headlines and the metaverse has seemingly been sidelined by Mark Zuckerberg’s decision to shift focus from the metaverse to AI, it’s important to note that the metaverse is still as relevant as ever.  \\nLearn More & See All Courses  \\nNew!  \\nHBR Learning  \\nDigital Intelligence Course',\n",
       " \"Learn More & See All Courses  \\nNew!  \\nHBR Learning  \\nDigital Intelligence Course  \\nAccelerate your career with Harvard ManageMentor®. HBR Learning’s online leadership training helps you hone your skills with courses like Digital Intelligence . Earn badges to share on LinkedIn and your resume. Access more than 40 courses trusted by Fortune 500 companies.  \\nExcel in a world that's being continually transformed by technology.  \\nStart Course  \\nRead more on Web-based technologies or related topic Innovation  \\nDP  \\nDeborah Perry Piscione is The New York Times best-selling author of Secrets of Silicon Valley\\xa0and co-founder of Work3 Institute,\\xa0an advisory and research firm on AI + web3 technologies. She is the co-author of a forthcoming book (HBR Press, 2024) on how disruptive technologies are revolutionizing the way we work.  \\nJD  \\nJosh Drean is co-founder of Work3 Institute, a web3 advisor at Harvard Innovation Labs, and co-founder of DreanMedia. His forthcoming book (HBR Press, 2024) explores how web3 and the metaverse can help design better employee experiences.  \\nTweet  \\nPost  \\nShare  \\nAnnotate  \\nSave  \\nGet PDF  \\nBuy Copies  \\nPrint  \\nNew!  \\nHBR Learning  \\nDigital Intelligence Course  \\nAccelerate your career with Harvard ManageMentor®. HBR Learning’s online leadership training helps you hone your skills with courses like Digital Intelligence . Earn badges to share on LinkedIn and your resume. Access more than 40 courses trusted by Fortune 500 companies.\",\n",
       " \"Excel in a world that's being continually transformed by technology.  \\nStart Course  \\nLearn More & See All Courses  \\nRead more on Web-based technologies or related topic Innovation  \\nPartner Center  \\nLatest Magazine Ascend Topics Podcasts Video Store The Big Idea Data & Visuals Case Selections HBR Learning  \\nSubscribe  \\nExplore HBR  \\nThe Latest All Topics Magazine Archive The Big Idea Reading Lists Case Selections Video Podcasts Webinars Data & Visuals My Library Newsletters HBR Press HBR Ascend  \\nHBR Store  \\nArticle Reprints Books Cases Collections Magazine Issues HBR Guide Series HBR 20-Minute Managers HBR Emotional Intelligence Series HBR Must Reads Tools  \\nAbout HBR  \\nContact Us Advertise with Us Information for Booksellers/Retailers Masthead Global Editions Media Inquiries Guidelines for Authors HBR Analytic Services Copyright Permissions  \\nManage My Account  \\nMy Library Topic Feeds Orders Account Settings Email Preferences Account FAQ Help Center Contact Customer Service  \\nFollow HBR  \\nFacebook Twitter LinkedIn Instagram Your Newsreader  \\nAbout Us Careers Privacy Policy Cookie Policy Copyright Information Trademark Policy  \\nHarvard Business Publishing:  \\nHigher Education Corporate Learning Harvard Business Review Harvard Business School  \\nCopyright © \\xa0 Harvard Business School Publishing. All rights reserved. Harvard Business Publishing is an affiliate of Harvard Business School.\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating embedding endpoint with MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploy-llm-llama-rag-vectorsearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
